# import the necessary python libraries
import numpy as np
import matplotlib.pyplot as plt


from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

''' Function to generate a toy dataset '''
def generate_toy_data(n_points=20, noise_std=0.25):

    x = np.linspace(-5, 5, n_points)

    #define a function relating input to output
    f = lambda x: 0.01 * x**3

    #generate noisy training labels
    y = f(x) + np.random.normal(0, noise_std, n_points)
    
    return x, y



# 0. generate a data set for doing regression
x, y = generate_toy_data() 

# split our data set into training and testing using a the 'train_test_split' function from sklearn
# see the documentation for 'train_test_split' here: 
# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)



# 1. Instantiate an Linear Regression model object
linear_regressor = LinearRegression()
# 2. Fit model to the training data
linear_regressor.fit(x_train.reshape(-1,1),y_train.reshape(-1,1))
# 3. Make prediction on test input data 
y_train_pred = linear_regressor.predict(x_train.reshape(-1,1))
# 4. Visualize results
f, ax = plt.subplots(1, 1, figsize=(10, 5)) # make a figure with one row and one column of size 10x5
ax.scatter(x_train, y_train) # scatter plot the training data
ax.plot(x_train, y_train_pred, 'r')  # plot the learned linear regression function by plotting the predictions
plt.show() # display the figure



# Hint: you can use a pre-defined MSE function in the library documentation at: 
# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html
#

## COMPLETE
training_mse = mean_squared_error(y_train, y_train_pred)

y_test_pred = linear_regressor.predict(x_test.reshape(-1,1))
test_mse = mean_squared_error(y_test, y_test_pred)
#
print('MSE on training data is: {}'.format(training_mse))
print('MSE on test data is: {}'.format(test_mse))


# Make a list of polynomial degrees we want to try
poly_degree = np.array([2, 4, 16])

# Visualize the three models in a figure with one row and 3 columns, with size 15x5
fig, ax = plt.subplots(1, 3, figsize=(15, 5))

# Iterate over the polynomial degrees and the subplots in the figure
for i in range(3):

  ax[i].scatter(x_train, y_train, color='blue', label='training data') # scatter plot the training data

  # Step 1: 
  # Instantiate a sklearn PolynomialFeatures model of degree=poly_degree[i] 
  # and transform both the training and the testing data to add polynomial features to our datasets.
  # (Hint: look for documentation on sklearn.preprocessing.PolynomialFeatures)
  ## COMPLETE
  poly = PolynomialFeatures(poly_degree[i])
  x_train_poly = poly.fit_transform(x_train.reshape(-1,1))
  x_test_poly = poly.fit_transform(x_test.reshape(-1,1))
  #
  # Step 2:
  # Instantiate a LinearRegression model, fit the model, and make predictions on training and test data
  # You should set fit_intercept=False for this LinearRegression model, since the intercept is already
  # generated by PolynomialFeatures
  ## COMPLETE
  #
  linear_regressor.fit(x_train_poly, y_train)
  y_train_pred = linear_regressor.predict(x_train_poly)
  y_test_pred = linear_regressor.predict(x_test_poly)

  sorted_indices = x_train.argsort()
  #
  # Step 3:
  # Plot the learned polynomial regression model by plotting the model prediction against the training input 
  # COMPLETE
  #
  ax[i].plot(x_train[sorted_indices], y_train_pred[sorted_indices], color='red', label='polynomial model')
  #
  # Step 4:
  # Compute and print the training and test MSE.
  # COMPLETE
  #
  mse_train = mean_squared_error(y_train, y_train_pred)
  mse_test = mean_squared_error(y_test, y_test_pred)
  #
  print('For degree {}, training MSE is: {:.3f}, test MSE is: {:.3f}'.format(poly_degree[i], mse_train, mse_test))

  ax[i].set_title('Degree = {}'.format(poly_degree[i])) # set the title of the ith subplot
  ax[i].legend(loc='best') # display the legend for the plot

plt.show() # display the figure

# Degree 16 is overfit: even though the training MSE is 0.000, the test MSE is ridiculously high -- the model is
# no longer appropriate for realistic data

np.random.seed(10)
# DATASET 1
x1, y1 = generate_toy_data(n_points=50, noise_std=0.1) # generate low noise data
x_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.33, random_state=42) #split data into training and test

# DATASET 2
x2, y2 = generate_toy_data(n_points=50, noise_std=0.4) # generate high noise data
x_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.33, random_state=42) #split data into training and test

# Visualize the two datasets in a figure with one row and 2 columns, with size 10x5
fig, ax = plt.subplots(1, 2, figsize=(10, 5))

ax[0].scatter(x_train1, y_train1, color='blue') # plot dataset 1
ax[1].scatter(x_train2, y_train2, color='blue') # plot dataset 2

ax[0].scatter(x_test1, y_test1, color='green') # plot dataset 1
ax[1].scatter(x_test2, y_test2, color='green') # plot dataset 2

ax[0].set_title('Dataset 1: low noise') # set the title for subplot 1
ax[1].set_title('Dataset 2: high noise') # set the title for subplot 2

plt.show() # display the figure



# Visualize the two models in a figure with one row and 2 columns, with size 10x5
fig, ax = plt.subplots(1, 2,figsize=(10, 5))

# Step 1:
# Instantiate a LinearRegression model, fit the model, and make predictions on training and test data
## COMPLETE
#
# Step 2:
# Plot the learned linear regression model by plotting the model prediction against the training input 
# COMPLETE
#
# ax[0].plot(x_train, y_train_pred, color='red', label='polynomial model')
#
# Step 3:
# Compute and print the training and test MSE.
# COMPLETE
#
# mse_train = ...
# mse_test = ...
#
# print('For low noise dataset, training MSE is: {:.3f}, test MSE is: {:.3f}'.format(mse_train, mse_test))

ax[0].scatter(x_train1, y_train1, color='blue') # plot dataset 1
ax[0].scatter(x_test1, y_test1, color='green') # plot dataset 1
ax[0].set_title('Low Noise Dataset')

# Step 1: 
# Instantiate a sklearn PolynomialFeatures model of degree=3 
# and transform both the training and the testing data to add polynomial features to our datasets.
# (Hint: look for documentation on sklearn.preprocessing.PolynomialFeatures)
## COMPLETE
#
# Step 2:
# Instantiate a LinearRegression model, fit the model, and make predictions on training and test data
## COMPLETE
#
# Step 3:
# Plot the learned polynomial regression model by plotting the model prediction against the training input 
# COMPLETE
#
# ax[1].plot(x_train, y_train_pred, color='red', label='polynomial model')
#
# Step 4:
# Compute and print the training and test MSE.
# COMPLETE
#
# mse_train = ...
# mse_test = ...
#
# print('For high noise dataset, training MSE is: {:.3f}, test MSE is: {:.3f}'.format(mse_train, mse_test))

ax[1].scatter(x_train2, y_train2, color='blue') # plot dataset 2
ax[1].scatter(x_test2, y_test2, color='green') # plot dataset 2
ax[1].set_title('High Noise Dataset')

plt.show()